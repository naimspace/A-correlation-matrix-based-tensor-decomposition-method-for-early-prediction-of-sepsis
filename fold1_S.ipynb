{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import glob\n",
    "import tensorly as tl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdata=pd.read_csv('imputed_data.csv') #Data imputed_data.csv prepared at matlab (1552210,41) with foward-fill imputation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths=pd.read_csv('lengths.csv',header=None) # Individual Length of Stay (LOS) fo all 40336 patients from taining data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Re-arranging imputed 1552210 hours of data in 40336 recods\n",
    "k=0;\n",
    "b=[]\n",
    "intr=lengths.iloc[:].values\n",
    "intr1=np.array(intr,dtype=None)\n",
    "x=range(0,40336)\n",
    "for n in x:\n",
    "    var=[]    \n",
    "    aa=intr1[n]\n",
    "    a2=aa[0]\n",
    "    var=pd.DataFrame(fdata.iloc[k:k+a2,:].values)\n",
    "    b.append(var)\n",
    "    k=k+a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naimahmed\\anaconda3\\lib\\site-packages\\tensorly\\backend\\core.py:779: RuntimeWarning: invalid value encountered in sqrt\n",
      "  S = np.where(np.abs(S) <= np.finfo(S.dtype).eps, 0, np.sqrt(S))\n",
      "C:\\Users\\Naimahmed\\anaconda3\\lib\\site-packages\\tensorly\\backend\\core.py:780: RuntimeWarning: invalid value encountered in less_equal\n",
      "  V = np.dot(matrix.T.conj(), U * np.where(np.abs(S) <= np.finfo(S.dtype).eps, 0, 1/S)[None, :])\n",
      "C:\\Users\\Naimahmed\\anaconda3\\lib\\site-packages\\tensorly\\backend\\core.py:780: RuntimeWarning: divide by zero encountered in true_divide\n",
      "  V = np.dot(matrix.T.conj(), U * np.where(np.abs(S) <= np.finfo(S.dtype).eps, 0, 1/S)[None, :])\n"
     ]
    }
   ],
   "source": [
    "#compute correlation of every 6 hours of data for 24 covariates(excluding covariates with \n",
    "#more than 95 % of missing values(10 covariates) and 6 demographics)\n",
    "# This cell corresponds to 80% of training data ~32222 patient records equivalent tp 1241759 hours of length of stay \n",
    "#out of 40336 patients. So this forms the training data for first fold\n",
    "\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.cp_tensor import cp_to_tensor, CPTensor\n",
    "feat2D = np.zeros((100,1241759))\n",
    "varr=0\n",
    "for rec in range(32222):\n",
    "    A=b[rec]\n",
    "    A2=A.drop(columns =[7 ,8 ,13, 14 ,16, 20, 22, 26, 27, 32 ,34 ,35,36 ,37 ,38, 39 ])\n",
    "\n",
    "    k=6\n",
    "    u=len(A2)\n",
    "    feat=[]\n",
    "    for j in range(u-5):  # Start appending matrices from 7th hour\n",
    "        var1=[]\n",
    "        corrd=[]\n",
    "        var1=A2.iloc[j:k,:]\n",
    "        corrd=var1.corr()       #6 x 24 ----> 24 x 24\n",
    "        corrd.fillna(0, inplace=True)\n",
    "    \n",
    "        k=k+1\n",
    "        feat.append(corrd)\n",
    "    \n",
    "    var2=feat[0]        #replicate the 7th frame (matrix) to first 6 frames\n",
    "    aa=[]\n",
    "    for k in range(5):\n",
    "        aa.append(var2)\n",
    "    \n",
    "    newfeat=aa+feat \n",
    "    unfld=[]\n",
    "    for ll in range(len(newfeat)):\n",
    "        v1=np.array(newfeat[ll])\n",
    "        v2=v1.reshape(576)           # 24x24 =576\n",
    "        unfld.append(v2)# shape is 54,576\n",
    "\n",
    "    npunfld=np.array(unfld).T #576,54\n",
    "    npunfld1=npunfld.reshape(24,24,len(newfeat))\n",
    "\n",
    "    #tensor formation\n",
    "    tensor = tl.tensor(npunfld1, dtype=tl.float64)\n",
    "    unfolded = tl.unfold(tensor, mode=0)\n",
    "    tl.fold(unfolded, mode=0, shape=tensor.shape)\n",
    "\n",
    "    # Apply Tucker decomposition\n",
    "    Tucker_tensor = tucker(tensor,[10,10,u]) # R1=10, R2=10, R3= length of stay.\n",
    "    coreT=Tucker_tensor[0]\n",
    "\n",
    "    fm=coreT.reshape(100,len(A2)) # reshape core for 100 features...!\n",
    "    feat2D[:,varr:varr+len(A2)]=fm\n",
    "    #print(rec)\n",
    "    varr=varr+len(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1241759, 100)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat2DT=feat2D.T\n",
    "feat2DT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute correlation of every 6 hours of data for 24 covariates(excluding covariates with \n",
    "#more than 59 % of missing values(10) and 6 demographics)\n",
    "\n",
    "from tensorly.decomposition import tucker\n",
    "from tensorly.decomposition import parafac\n",
    "from tensorly.cp_tensor import cp_to_tensor, CPTensor\n",
    "feat2Dtest = np.zeros((100,310451))\n",
    "varr=0\n",
    "for rec in range(32222,40336):\n",
    "    A=b[rec]\n",
    "    A2=A.drop(columns =[7 ,8 ,13, 14 ,16, 20, 22, 26, 27, 32 ,34 ,35,36 ,37 ,38, 39])\n",
    "\n",
    "    k=6\n",
    "    u=len(A2)\n",
    "    feat=[]\n",
    "    for j in range(u-5):  # Start appending matrices from 7th hour\n",
    "        var1=[]\n",
    "        corrd=[]\n",
    "        var1=A2.iloc[j:k,:]\n",
    "        corrd=var1.corr()       #6 x 24 ----> 24 x 24\n",
    "        corrd.fillna(0, inplace=True)\n",
    "    \n",
    "        k=k+1\n",
    "        feat.append(corrd)\n",
    "    \n",
    "    var2=feat[0]        #replicate the 7th frame (matrix) to first 6 frames\n",
    "    aa=[]\n",
    "    for k in range(5):\n",
    "        aa.append(var2)\n",
    "    \n",
    "    newfeat=aa+feat \n",
    "    unfld=[]\n",
    "    for ll in range(len(newfeat)):\n",
    "        v1=np.array(newfeat[ll])\n",
    "        v2=v1.reshape(576)           # 24x24 =576\n",
    "        unfld.append(v2)# shape is 54,576\n",
    "\n",
    "    npunfld=np.array(unfld).T #576,54\n",
    "    npunfld1=npunfld.reshape(24,24,len(newfeat))\n",
    "\n",
    "    #tensor formation\n",
    "    tensor_test = tl.tensor(npunfld1, dtype=tl.float64)\n",
    "    unfolded = tl.unfold(tensor_test, mode=0)\n",
    "    tl.fold(unfolded, mode=0, shape=tensor_test.shape)\n",
    "\n",
    "    # Apply Tucker decomposition\n",
    "    Tucker_tensor_test = tucker(tensor_test,[10,10,u]) # R1=10, R2=10, R3= length of stay.\n",
    "    coreT_test=Tucker_tensor_test[0]\n",
    "\n",
    "    fm_test=coreT_test.reshape(100,len(A2)) # rehape core for 100 features...!\n",
    "    feat2Dtest[:,varr:varr+len(A2)]=fm_test\n",
    "    #print(rec)\n",
    "    varr=varr+len(A2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sepsis_data1 = pd.read_csv('labels_sir.csv')\n",
    "Y=sepsis_data1.values\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindata1=feat2DT\n",
    "traindata1.shape\n",
    "trainlabels=Y[0:1241759];\n",
    "trainlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310451, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdata1=feat2Dtest.T\n",
    "testdata1.shape\n",
    "testlabels=Y[1241759:1552210];\n",
    "testlabels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>range1</th>\n",
       "      <th>range2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55</td>\n",
       "      <td>77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>78</td>\n",
       "      <td>125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>126</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>155</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40331</th>\n",
       "      <td>1552034</td>\n",
       "      <td>1552081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40332</th>\n",
       "      <td>1552082</td>\n",
       "      <td>1552106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40333</th>\n",
       "      <td>1552107</td>\n",
       "      <td>1552155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40334</th>\n",
       "      <td>1552156</td>\n",
       "      <td>1552175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40335</th>\n",
       "      <td>1552176</td>\n",
       "      <td>1552210</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40336 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        range1   range2\n",
       "0            1       54\n",
       "1           55       77\n",
       "2           78      125\n",
       "3          126      154\n",
       "4          155      202\n",
       "...        ...      ...\n",
       "40331  1552034  1552081\n",
       "40332  1552082  1552106\n",
       "40333  1552107  1552155\n",
       "40334  1552156  1552175\n",
       "40335  1552176  1552210\n",
       "\n",
       "[40336 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = pd.read_csv('range1.csv')\n",
    "dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[     1,     37],\n",
       "       [    38,     83],\n",
       "       [    84,    134],\n",
       "       ...,\n",
       "       [310348, 310396],\n",
       "       [310397, 310416],\n",
       "       [310417, 310451]], dtype=int64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range_test=range(32223:end,:)-1241759;\n",
    "range_test=dummy.iloc[32222:40336,:].values\n",
    "range_test1=range_test-1241759\n",
    "range_test1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32222        37\n",
       "32223        83\n",
       "32224       134\n",
       "32225       142\n",
       "32226       150\n",
       "          ...  \n",
       "40331    310322\n",
       "40332    310347\n",
       "40333    310396\n",
       "40334    310416\n",
       "40335    310451\n",
       "Name: range2, Length: 8114, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#range_test=range(32223:end,:)-1241759;\n",
    "range_test=dummy[32222:40336]\n",
    "range_test1=range_test-1241759\n",
    "a=range_test1.iloc[:,1]\n",
    "a\n",
    "#range_test2=range_test1[:,0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "b=[]\n",
    "x=range(0,8114)\n",
    "for n in x:\n",
    "      b.append(a[n+32222])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, os, os.path, sys, warnings\n",
    "\n",
    "def evaluate_sepsis_score(label_directory, prediction_directory,num_files):\n",
    "    # Set parameters.\n",
    "    dt_early   = -12\n",
    "    dt_optimal = -6\n",
    "    dt_late    = 3\n",
    "\n",
    "    max_u_tp = 1\n",
    "    min_u_fn = -2\n",
    "    u_fp     = -0.05\n",
    "    u_tn     = 0\n",
    "\n",
    "    \n",
    "    # Compute utility.\n",
    "    observed_utilities = np.zeros(num_files)\n",
    "    best_utilities     = np.zeros(num_files)\n",
    "    worst_utilities    = np.zeros(num_files)\n",
    "    inaction_utilities = np.zeros(num_files)\n",
    "\n",
    "    for k in range(num_files):\n",
    "        labels = cohort_labels[k]\n",
    "        num_rows          = len(labels)\n",
    "        observed_predictions = cohort_predictions[k]\n",
    "        best_predictions     = np.zeros(num_rows)\n",
    "        worst_predictions    = np.zeros(num_rows)\n",
    "        inaction_predictions = np.zeros(num_rows)\n",
    "\n",
    "        if np.any(labels):\n",
    "            t_sepsis = np.argmax(labels) - dt_optimal\n",
    "            best_predictions[max(0, t_sepsis + dt_early) : min(t_sepsis + dt_late + 1, num_rows)] = 1\n",
    "        worst_predictions = 1 - best_predictions\n",
    "\n",
    "        observed_utilities[k] = compute_prediction_utility(labels, observed_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        best_utilities[k]     = compute_prediction_utility(labels, best_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        worst_utilities[k]    = compute_prediction_utility(labels, worst_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "        inaction_utilities[k] = compute_prediction_utility(labels, inaction_predictions, dt_early, dt_optimal, dt_late, max_u_tp, min_u_fn, u_fp, u_tn)\n",
    "\n",
    "    unnormalized_observed_utility = np.sum(observed_utilities)\n",
    "    unnormalized_best_utility     = np.sum(best_utilities)\n",
    "    unnormalized_worst_utility    = np.sum(worst_utilities)\n",
    "    unnormalized_inaction_utility = np.sum(inaction_utilities)\n",
    "\n",
    "    normalized_observed_utility = (unnormalized_observed_utility - unnormalized_inaction_utility) / (unnormalized_best_utility - unnormalized_inaction_utility)\n",
    "\n",
    "    return  normalized_observed_utility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_auc(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not 0 <= prediction <= 1:\n",
    "                warnings.warn('Predictions do not satisfy 0 <= prediction <= 1.')\n",
    "\n",
    "    # Find prediction thresholds.\n",
    "    thresholds = np.unique(predictions)[::-1]\n",
    "    if thresholds[0] != 1:\n",
    "        thresholds = np.insert(thresholds, 0, 1)\n",
    "    if thresholds[-1] == 0:\n",
    "        thresholds = thresholds[:-1]\n",
    "\n",
    "    n = len(labels)\n",
    "    m = len(thresholds)\n",
    "\n",
    "    # Populate contingency table across prediction thresholds.\n",
    "    tp = np.zeros(m)\n",
    "    fp = np.zeros(m)\n",
    "    fn = np.zeros(m)\n",
    "    tn = np.zeros(m)\n",
    "\n",
    "    # Find indices that sort the predicted probabilities from largest to\n",
    "    # smallest.\n",
    "    idx = np.argsort(predictions)[::-1]\n",
    "\n",
    "    i = 0\n",
    "    for j in range(m):\n",
    "        # Initialize contingency table for j-th prediction threshold.\n",
    "        if j == 0:\n",
    "            tp[j] = 0\n",
    "            fp[j] = 0\n",
    "            fn[j] = np.sum(labels)\n",
    "            tn[j] = n - fn[j]\n",
    "        else:\n",
    "            tp[j] = tp[j - 1]\n",
    "            fp[j] = fp[j - 1]\n",
    "            fn[j] = fn[j - 1]\n",
    "            tn[j] = tn[j - 1]\n",
    "\n",
    "        # Update contingency table for i-th largest predicted probability.\n",
    "        while i < n and predictions[idx[i]] >= thresholds[j]:\n",
    "            if labels[idx[i]]:\n",
    "                tp[j] += 1\n",
    "                fn[j] -= 1\n",
    "            else:\n",
    "                fp[j] += 1\n",
    "                tn[j] -= 1\n",
    "            i += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    tpr = np.zeros(m)\n",
    "    tnr = np.zeros(m)\n",
    "    ppv = np.zeros(m)\n",
    "    npv = np.zeros(m)\n",
    "\n",
    "    for j in range(m):\n",
    "        if tp[j] + fn[j]:\n",
    "            tpr[j] = tp[j] / (tp[j] + fn[j])\n",
    "        else:\n",
    "            tpr[j] = 1\n",
    "        if fp[j] + tn[j]:\n",
    "            tnr[j] = tn[j] / (fp[j] + tn[j])\n",
    "        else:\n",
    "            tnr[j] = 1\n",
    "        if tp[j] + fp[j]:\n",
    "            ppv[j] = tp[j] / (tp[j] + fp[j])\n",
    "        else:\n",
    "            ppv[j] = 1\n",
    "        if fn[j] + tn[j]:\n",
    "            npv[j] = tn[j] / (fn[j] + tn[j])\n",
    "        else:\n",
    "            npv[j] = 1\n",
    "\n",
    "    # Compute AUROC as the area under a piecewise linear function with TPR /\n",
    "    # sensitivity (x-axis) and TNR / specificity (y-axis) and AUPRC as the area\n",
    "    # under a piecewise constant with TPR / recall (x-axis) and PPV / precision\n",
    "    # (y-axis).\n",
    "    auroc = 0\n",
    "    auprc = 0\n",
    "    for j in range(m-1):\n",
    "        auroc += 0.5 * (tpr[j + 1] - tpr[j]) * (tnr[j + 1] + tnr[j])\n",
    "        auprc += (tpr[j + 1] - tpr[j]) * ppv[j + 1]\n",
    "\n",
    "    return auroc, auprc\n",
    "\n",
    "# The compute_accuracy_f_measure function computes the accuracy and F-measure\n",
    "# for a patient.\n",
    "#\n",
    "# Inputs:\n",
    "#   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "#   septic at time i.\n",
    "#\n",
    "#   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "#   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "#   patient is predicted to be septic at time i.  Note that there must be a\n",
    "#   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "#\n",
    "# Output:\n",
    "#   'accuracy' is a scalar that gives the accuracy of the predictions using its\n",
    "#   binarized predictions.\n",
    "#\n",
    "#   'f_measure' is a scalar that gives the F-measure of the predictions using its\n",
    "#   binarized predictions.\n",
    "#\n",
    "# Example:\n",
    "#   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "#   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "#   In [3]: accuracy, f_measure = compute_accuracy_f_measure(labels, predictions)\n",
    "#   In [4]: accuracy\n",
    "#   Out[4]: 0.666666666667\n",
    "#   In [5]: f_measure\n",
    "#   Out[5]: 0.666666666667\n",
    "\n",
    "def compute_accuracy_f_measure(labels, predictions, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "    # Populate contingency table.\n",
    "    n = len(labels)\n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    tn = 0\n",
    "\n",
    "    for i in range(n):\n",
    "        if labels[i] and predictions[i]:\n",
    "            tp += 1\n",
    "        elif not labels[i] and predictions[i]:\n",
    "            fp += 1\n",
    "        elif labels[i] and not predictions[i]:\n",
    "            fn += 1\n",
    "        elif not labels[i] and not predictions[i]:\n",
    "            tn += 1\n",
    "\n",
    "    # Summarize contingency table.\n",
    "    if tp + fp + fn + tn:\n",
    "        accuracy = float(tp + tn) / float(tp + fp + fn + tn)\n",
    "    else:\n",
    "        accuracy = 1.0\n",
    "\n",
    "    if 2 * tp + fp + fn:\n",
    "        f_measure = float(2 * tp) / float(2 * tp + fp + fn)\n",
    "    else:\n",
    "        f_measure = 1.0\n",
    "\n",
    "    return accuracy, f_measure\n",
    "\n",
    "# The compute_prediction_utility function computes the total time-dependent\n",
    "# utility for a patient.\n",
    "#\n",
    "# Inputs:\n",
    "#   'labels' is a binary vector, where labels[i] == 0 if the patient is not\n",
    "#   labeled as septic at time i and labels[i] == 1 if the patient is labeled as\n",
    "#   septic at time i.\n",
    "#\n",
    "#   'predictions' is a binary vector, where predictions[i] == 0 if the patient\n",
    "#   is not predicted to be septic at time i and predictions[i] == 1 if the\n",
    "#   patient is predicted to be septic at time i.  Note that there must be a\n",
    "#   prediction for every label, i.e, len(labels) == len(predictions).\n",
    "#\n",
    "# Output:\n",
    "#   'utility' is a scalar that gives the total time-dependent utility of the\n",
    "#   algorithm using its binarized predictions.\n",
    "#\n",
    "# Example:\n",
    "#   In [1]: labels = [0, 0, 0, 0, 1, 1]\n",
    "#   In [2]: predictions = [0, 0, 1, 1, 1, 1]\n",
    "#   In [3]: utility = compute_prediction_utility(labels, predictions)\n",
    "#   In [4]: utility\n",
    "#   Out[4]: 3.388888888888889\n",
    "\n",
    "def compute_prediction_utility(labels, predictions, dt_early=-12, dt_optimal=-6, dt_late=3.0, max_u_tp=1, min_u_fn=-2, u_fp=-0.05, u_tn=0, check_errors=True):\n",
    "    # Check inputs for errors.\n",
    "    if check_errors:\n",
    "        if len(predictions) != len(labels):\n",
    "            raise Exception('Numbers of predictions and labels must be the same.')\n",
    "\n",
    "        for label in labels:\n",
    "            if not label in (0, 1):\n",
    "                raise Exception('Labels must satisfy label == 0 or label == 1.')\n",
    "\n",
    "        for prediction in predictions:\n",
    "            if not prediction in (0, 1):\n",
    "                raise Exception('Predictions must satisfy prediction == 0 or prediction == 1.')\n",
    "\n",
    "        if dt_early >= dt_optimal:\n",
    "            raise Exception('The earliest beneficial time for predictions must be before the optimal time.')\n",
    "\n",
    "        if dt_optimal >= dt_late:\n",
    "            raise Exception('The optimal time for predictions must be before the latest beneficial time.')\n",
    "\n",
    "    # Does the patient eventually have sepsis?\n",
    "    if np.any(labels):\n",
    "        is_septic = True\n",
    "        t_sepsis = np.argmax(labels) - dt_optimal\n",
    "    else:\n",
    "        is_septic = False\n",
    "        t_sepsis = float('inf')\n",
    "\n",
    "    n = len(labels)\n",
    "\n",
    "    # Define slopes and intercept points for utility functions of the form\n",
    "    # u = m * t + b.\n",
    "    m_1 = float(max_u_tp) / float(dt_optimal - dt_early)\n",
    "    b_1 = -m_1 * dt_early\n",
    "    m_2 = float(-max_u_tp) / float(dt_late - dt_optimal)\n",
    "    b_2 = -m_2 * dt_late\n",
    "    m_3 = float(min_u_fn) / float(dt_late - dt_optimal)\n",
    "    b_3 = -m_3 * dt_optimal\n",
    "\n",
    "    # Compare predicted and true conditions.\n",
    "    u = np.zeros(n)\n",
    "    for t in range(n):\n",
    "        if t <= t_sepsis + dt_late:\n",
    "            # TP\n",
    "            if is_septic and predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = max(m_1 * (t - t_sepsis) + b_1, u_fp)\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_2 * (t - t_sepsis) + b_2\n",
    "            # FP\n",
    "            elif not is_septic and predictions[t]:\n",
    "                u[t] = u_fp\n",
    "            # FN\n",
    "            elif is_septic and not predictions[t]:\n",
    "                if t <= t_sepsis + dt_optimal:\n",
    "                    u[t] = 0\n",
    "                elif t <= t_sepsis + dt_late:\n",
    "                    u[t] = m_3 * (t - t_sepsis) + b_3\n",
    "            # TN\n",
    "            elif not is_septic and not predictions[t]:\n",
    "                u[t] = u_tn\n",
    "\n",
    "    # Find total utility for patient.\n",
    "    return np.sum(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_labels        = []\n",
    "cohort_predictions   = []\n",
    "cohort_probabilities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = {\n",
    "            #'max_features':  hp.choice('max_features',['sqrt','log2', 0.5,0.75]), \n",
    "            'num_leaves': (40, 100), \n",
    "            'reg_lambda': (0, 10),\n",
    "            'lambda_l1': (0, 5),\n",
    "            'lambda_l2': (5, 10),\n",
    "            'max_depth': (10,20),\n",
    "            'scale_pos_weight':(18,40),\n",
    "            'learning_rate':(0.05,0.2),\n",
    "            'min_data_in_leaf':(40,100),\n",
    "            'reg_alpha': (0, 10),\n",
    "            #'min_split_gain':(0.01,10)\n",
    "    \n",
    "            \n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(num_leaves,\n",
    "             reg_lambda,\n",
    "             lambda_l1,\n",
    "             lambda_l2,\n",
    "             max_depth,\n",
    "             scale_pos_weight,\n",
    "             learning_rate,\n",
    "             min_data_in_leaf,\n",
    "             reg_alpha):\n",
    "             #min_split_gain):\n",
    "        num_leaves = int(num_leaves)\n",
    "        min_data_in_leaf = int(min_data_in_leaf)\n",
    "        max_depth = int(max_depth)\n",
    "        #n_estimators =int(n_estimators)\n",
    "        params = {\n",
    "           # 'max_features': space['max_features'],\n",
    "            'num_leaves':num_leaves,\n",
    "            'reg_lambda':reg_lambda,\n",
    "            'lambda_l1':lambda_l1,\n",
    "            'lambda_l2':lambda_l2,\n",
    "            'max_depth':max_depth,\n",
    "            'scale_pos_weight':scale_pos_weight,\n",
    "            'learning_rate':learning_rate,\n",
    "            'min_data_in_leaf': min_data_in_leaf,\n",
    "            'reg_alpha': reg_alpha\n",
    "            #'min_split_gain':min_split_gain\n",
    "             }\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(traindata1, trainlabels, test_size=0.2, random_state=42)\n",
    "        import lightgbm as lgb\n",
    "        clf = lgb.LGBMClassifier(n_estimators=100, is_unbalanced=True, **params)\n",
    "        clf.fit(traindata1, trainlabels)#,early_stopping_rounds=100, eval_metric=\"f1_score\",\n",
    "        #eval_set=[(traindata1, trainlabels), (testdata1, testlabels)])\n",
    "        pred_scores = clf.predict_proba(testdata1)\n",
    "        pred_scores2=pred_scores[:,1]\n",
    "        y_pred=clf.predict(testdata1)\n",
    "        n=0\n",
    "        m=0\n",
    "        for k in b:\n",
    "            cohort_labels.append(testlabels[m:k])\n",
    "            cohort_predictions.append(y_pred[m:k])\n",
    "            cohort_probabilities.append(pred_scores2[m:k])\n",
    "            m=b[n]\n",
    "            n=n+1\n",
    "        \n",
    "        normalized_observed_utility = evaluate_sepsis_score(cohort_labels,cohort_predictions,len(cohort_labels));\n",
    "        return normalized_observed_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from bayes_opt import BayesianOptimization\n",
    "LGB_BO = BayesianOptimization(objective, space)\n",
    "print(LGB_BO.space.keys)\n",
    "import warnings\n",
    "init_points = 16\n",
    "n_iter = 16\n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings('ignore')    \n",
    "LGB_BO.maximize(init_points=init_points, n_iter=n_iter, acq='ucb', xi=0.0, alpha=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Naimahmed\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:235: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\Naimahmed\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:268: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', lambda_l1=4.044, lambda_l2=6.72,\n",
       "               learning_rate=0.08874, max_depth=17, min_child_samples=20,\n",
       "               min_child_weight=0.001, min_data_in_leaf=52, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_leaves=45, objective=None,\n",
       "               random_state=None, reg_alpha=9.005, reg_lambda=6.675,\n",
       "               scale_pos_weight=23.13, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "#model = XGBClassifier(learning_rate=0.01, n_estimators=100, subsample=0.8, max_depth=3,objective='binary:logistic', reg_alpha=0.3,colsample_bytree=0.4,gamma=10,scale_pos_weight= 20)\n",
    "#model.fit(traindata1, trainlabels)\n",
    "\n",
    "#model = lgb.LGBMClassifier(num_leaves=60,min_data_in_leaf= 120,max_depth= 1,learning_rate= 0.01,reg_alpha= 0, reg_lambda= 0,metric= 'f1_score', scale_pos_weight= 20)\n",
    "model = lgb.LGBMClassifier(lambda_l1= 4.044,lambda_l2= 6.72,learning_rate= 0.08874, max_depth =17,min_data_in_leaf=52,num_leaves=45,reg_alpha=9.005,reg_lambda= 6.675,scale_pos_weight=23.13)\n",
    "model.fit(traindata1, trainlabels)\n",
    "#1.938    |  7.643    |  0.08446  |  15.6     |  47.77    |  55.42    |  1.944    |  3.547    |  27.67    |              U36.75\n",
    "#0.363    |  1.083    |  8.687    |  0.05214  |  14.06    |  65.66    |  65.88    |  8.095    |  4.26     |  24.9     |  U37.2\n",
    "#0.3684   |  4.044    |  6.72     |  0.08874  |  16.15    |  51.02    |  44.16    |  9.005    |  6.675    |  23.13    |  U37.66\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.9981466 , 0.0018534 ],\n",
       "       [0.97989952, 0.02010048],\n",
       "       [0.99393019, 0.00606981],\n",
       "       ...,\n",
       "       [0.54618489, 0.45381511],\n",
       "       [0.48391564, 0.51608436],\n",
       "       [0.37977986, 0.62022014]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores = model.predict_proba(testdata1)\n",
    "pred_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310451,)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_scores2=pred_scores[:,1]\n",
    "pred_scores2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(310451,)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred=model.predict(testdata1)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "cohort_labels        = []\n",
    "cohort_predictions   = []\n",
    "cohort_probabilities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=0\n",
    "m=0\n",
    "for k in b:\n",
    "    cohort_labels.append(testlabels[m:k])\n",
    "    cohort_predictions.append(y_pred[m:k])\n",
    "    cohort_probabilities.append(pred_scores2[m:k])\n",
    "    m=b[n]\n",
    "    n=n+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3766669128508125"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalized_observed_utility = evaluate_sepsis_score(cohort_labels,cohort_predictions,len(cohort_labels));\n",
    "normalized_observed_utility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
